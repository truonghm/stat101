[
["index.html", "Giới thiệu cơ bản về XSTK Preface", " Giới thiệu cơ bản về XSTK Truong Hoang November 18, 2020 Preface This document is my studying notes for the subject of Statistical Methodologies. I intend this to be the first part of my learning journey, which I call STAT101. Please note the emphasis on studying notes, which means that this is personal and not meant for distribution for other people to use. I’m not a qualified educator in any ways, so if you happen upon this page, please keep that in mind. The entire document is in Vietnamese, with this page being the only exception. I like to start with English, for personal reasons. I use several books to study the subject. Please find the full list below. Everything is built with bookdown in R and uploaded to Github Pages. "],
["motivation.html", "Motivation", " Motivation I admit it, I have a rather slow mind for learning. This could either stem from my (undiagnosed) ADHD, or my horrible work ethic, and obviously it feels better to blame on the ADHD. But be it the laziness or the inability to focus, I still have to find a way to learn efficiently, such that knowledge flows into my mind coherently, with every node of information inter-connected. This does not mean that the learning process gets easier, as I have just stated, but to make it more efficient: so that things I learn stay in my head for as long as possible. This is difficult by nature, because for the average person, there would certainly be topics that could not be fully understood right away. Learning is connecting the dots, and some dots can only be connected much later. I write these notes with a goal in mind: to make things as intuitively as possible, meaning that the flow of things should be smooth and efficient. To do this, certain concepts will be mentioned briefly, and then accepted as is, without further proof or explanation; we will come back to those in later parts. I hope that after I’m done with writing this, I would realize that I will have to go back and change a lot of things. Or, the better alternative would be that I still go back and change things, but after each chapter, when I become a little less ignorant. I believe life has meanings, and the noblest of all is that we live to change the world, no matter how small that effort might be. I think statistics is great tool for such task, because life is nothing more than chances. But let’s be clear here: I’m not trying to control “fate” (if it even exists at all). If anything, I’m trying to learn to accept fate, because I have came to understand that the best things in life can happen when you think less and believe more. I met my girlfriend this way, and she is the greatest fortune of my life, so why should I try to outsmart the man upstair? "],
["draft-outline.html", "1 Draft outline", " 1 Draft outline Thống kê mô tả cơ bản (basic descriptive statistics) Xác suất (probability) Sử dụng p/p Bootstrap để tìm khoảng tin cậy (Using bootstrap to find confidence interval) "],
["to-do.html", "2 To-do", " 2 To-do Citation!!! "],
["references.html", "3 References", " 3 References Mathematical Statistics with Resampling and R - Laura M. Chihara, Tim C. Hesterberg OpenIntro Statistics - Christopher D. Barr, David M. Diez, and Mine Çetinkaya-Rundel An Introduction to Statistical Methods &amp; Data Analysis - Lyman Ott, Michael Longnecker "],
["th-ng-kê-mô-t-c-b-n.html", "4 Thống kê mô tả cơ bản 4.1 Tóm tắt dữ liệu qua đồ thị 4.2 Xu hướng trung tâm 4.3 Mức độ phân tán (Spread/Dispersion) 4.4 Tiếp tục với biểu đồ 4.5 Tổng hợp thuật ngữ 4.6 Một số bài tập 4.7 Tham khảo", " 4 Thống kê mô tả cơ bản # load sẵn ggplot2 (đã bao gồm trong tidyverse) library(tidyverse, quietly = TRUE) Summary statistics (thống kê mô tả tóm tắt) là một phần cơ bản trong khoa học thống kê, giúp chúng ta mô tả ngắn gọn dữ liệu thông qua các chỉ số thống kê cơ bản nhất. Đồng thời, chúng ta cũng sử dụng các công cụ visualization trong R để đưa data trở thành 1 biểu đồ mang mục đích tóm tắt. Trong phần này, chúng ta sẽ sử dụng dataset TenMileRace từ package mosaicData. # Cài package nếu chưa có sẵn install.packages(&#39;mosaicData&#39;) data(TenMileRace, package=&#39;mosaicData&#39;) summary(TenMileRace) ## state time net age sex ## VA :3689 Min. : 2816 Min. : 2814 Min. :10.00 F:4325 ## MD :2166 1st Qu.: 5125 1st Qu.: 4950 1st Qu.:28.00 M:4311 ## DC :1642 Median : 5806 Median : 5555 Median :35.00 ## PA : 273 Mean : 5813 Mean : 5599 Mean :36.86 ## NY : 211 3rd Qu.: 6455 3rd Qu.: 6169 3rd Qu.:44.00 ## NJ : 92 Max. :10603 Max. :10536 Max. :87.00 ## (Other): 563 head(TenMileRace) ## state time net age sex ## 1 VA 6060 5978 12 M ## 2 MD 4515 4457 13 M ## 3 VA 5026 4928 13 M ## 4 MD 4229 4229 14 M ## 5 MD 5293 5076 14 M ## 6 VA 6234 5968 14 M Dataset này có các cột sau: state: Là nơi sống của người chạy. time: Thời gian chính thức từ lúc bắt đầu chạy đến lúc chạm vạch đích (tính bằng giây). net: Thời gian ghi được (theo giây) từ lúc người chạy vượt qua vạch bắt đầu cho đến lúc chạm vạch đích. Giá trị này thường nhỏ hơn giá trị time do có một số lượng lớn người tham gia chạy, và không thể sắp xếp cho người tham gia đứng thẳng hàng để xuất phát cùng 1 lúc được (need citation here). age: Tuổi của người tham gia chạy tính bằng năm. sex: Giới tính người tham gia chạy, thuộc dạng factor với 2 giá trị F (female) vàdư M (male). Trước tiên, chúng ta cần phân biệt giữa 2 loại dữ liệu: Discrete/Categorical (dữ liệu rời rạc): Là kiểu dữ liệu có giá trị nằm trong 1 tập hợp giới hạn có thể đếm được. Nếu dữ liệu là kiểu numeric, ta có thể gọi là Discrete, còn nếu là kiểu factor/string, ta gọi là Categorical. Ví dụ: Xếp hạng điểm của học sinh theo kiểu chữ cái (A/B/C/D/E/F) hoặc giới tính (Nam hoặc Nữ), hoặc số bữa ăn trong ngày (1 đến 3 bữa, thậm chí nhiều hơn, nhưng quan trọng là chúng ta có thể đếm được). Continuous (dữ liệu liên tục): Là kiểu dữ liệu số có thể nhận một số giá trị vô hạn, không thể đếm được. Ví dụ: Chiều cao, cân nặng, thời gian/tuổi (ví dụ: Có thể nói là 10 tuổi, nhưng cũng có thể nói là 10 tuổi 5 tháng 20 ngày 7 phút 5 giây, vân vân). Tóm lại, khi làm việc với dữ liệu, chúng ta có thể tự đặt câu hỏi: “Liệu có thể chia các giá trị trong dataset này thành các phần nhỏ hơn hay không?”. Nếu có, dữ liệu sẽ thuộc kiểu liên tục. 4.1 Tóm tắt dữ liệu qua đồ thị 4.1.1 Một biến rời rạc (univariate) Chúng ta có thể dùng bar chart để mô tả phân phối của một biến rời rạc (đơn biến - univariate). Các cột (bar) được vẽ thể hiện một giá trị (level) của nhóm factor, và chiều cao của cột (nếu để biểu đồ dọc) thể hiện số quan sát (observation). Ta cũng có thể dùng group_by và summarise để diễn giải bar chart như ở dưới: TenMileRace %&gt;% group_by(sex) %&gt;% summarise(count = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## sex count ## &lt;fct&gt; &lt;int&gt; ## 1 F 4325 ## 2 M 4311 ggplot(TenMileRace, aes(x=sex)) + geom_bar() Có thể thấy 2 cột có chiều cao tương đương nhau do số observation của mỗi giá trị factor gần bằng nhau (4325 vs. 4311). Như vậy, dữ liệu về người chạy đua trong data rất cân bằng về giới tính. 4.1.2 Một biến liên tục (univariate) Đồ thị histogram nhìn khá giống với đồ thị bar chart, nhưng vì được sử dụng với biến liên tục nên các cột sẽ chạm vào nhau. ggplot(TenMileRace, aes(x=net)) + geom_histogram() Chiều cao của mỗi cột trong đồ thị histogram thể hiện tần suất kết quả chạy theo các khoảng (interval), và trong R gọi là các bin (ngăn). Mặc định của hàm geom_histogram là bins=30, tức là mỗi khoảng cách nhau 30 đơn vị. Ta có thể diễn giải lại biểu đồ trên dưới dạng bảng: TenMileRace %&gt;% mutate(bins=cut(net, breaks=30)) %&gt;% group_by(bins) %&gt;% summarise(count=n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 29 x 2 ## bins count ## &lt;fct&gt; &lt;int&gt; ## 1 (2.81e+03,3.07e+03] 18 ## 2 (3.07e+03,3.33e+03] 22 ## 3 (3.33e+03,3.59e+03] 65 ## 4 (3.59e+03,3.84e+03] 131 ## 5 (3.84e+03,4.1e+03] 197 ## 6 (4.1e+03,4.36e+03] 339 ## 7 (4.36e+03,4.62e+03] 495 ## 8 (4.62e+03,4.87e+03] 685 ## 9 (4.87e+03,5.13e+03] 735 ## 10 (5.13e+03,5.39e+03] 1004 ## # … with 19 more rows các interval trong bảng và đồ thị trên đều có mặc định là \\((x, y]\\), tức là \\(x &lt; values &lt;= y\\) (right-closed, left-opened). Chú ý: Đồ thị histogram dựa vào diện tích của các cột, không phải chiều cao, tức là bằng \\(bin\\;width\\;*\\;count\\). 4.1.3 Một biến rời rạc và một biến liên tục (bivariate) Chúng ta thường phải so sánh mức độ phản hồi (dữ liệu liên tục) của hai biến rời rạc hoặc nhiều hơn. Biểu đồ boxplot thường để sử dụng để làm việc này. Mỗi observation gắn với một giá trị liên tục và 1 giá trị rời rạc ggplot(TenMileRace, aes(x=sex, y=net)) + geom_boxplot() Tron biểu đồ trên, phần rìa (hinge) của hộp được định nghĩa là các khoảng phần tư thứ nhất và thứ 3 (1st quartile, tương đương với 25% quantile, và 3rd quartile, đương tương với 75% quantile). Nói cách khác, 25% dữ liệu nằm dưới hộp, 50% nằm trong hộp, và phần 25% cuối cùng nằm phía trên hộp. Các điểm bên ngoài hộp là các outliers (điểm ngoại lai). Nếu định nghĩa Inter-Quartile Range (IQR) là chiều dài của hộp thì tất các quan sát lớn hơn 1.5*IQR tính từ hộp được coi như là 1 outlier. Ngoài cách trên, chúng ta có thể dùng 2 đồ thị histogram đặt cạnh nhau để so sánh: ggplot(TenMileRace, aes(x=net)) + geom_histogram() + facet_grid( . ~ sex ) Trong trường hợp này, đồ thị đặt theo chiều dọc sẽ dễ so sánh hơn: ggplot(TenMileRace, aes(x=net)) + geom_histogram() + facet_grid( sex ~ . ) # thay đổi chiều facet 4.1.4 Hai biến liên tục (bivariate) Để biểu diễn mối quan hệ của 2 biến liên tục, ta dùng biểu đô dạng điểm: ggplot(TenMileRace, aes(x=age, y=net, color=sex)) + geom_point() 4.2 Xu hướng trung tâm Ngoài việc sử dụng các loại biểu đồ và đồ thị, chúng ta còn có thể dùng các 1 số chỉ số cơ bản để miêu tả dữ liệu. Mở đầu là các chỉ số về chiều hướng trung tâm (centrality). 4.2.1 Mean (Trung bình cộng) Giá trị trung bình cộng (arithmetic mean, hay còn được gọi là average) là chỉ số cơ bản nhất, được tính theo công thức: \\[\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}=\\frac{1}{n}\\left(x_{1}+x_{2}+\\dots+x_{n}\\right)\\] Trong R, ta tính mean bằng cách dùng hàm mean(): # Dùng base R mean(TenMileRace$net) ## [1] 5599.065 # Dùng dplyr summarise(TenMileRace, mean(net)) ## mean(net) ## 1 5599.065 4.2.2 Median (Trung vị) Median (trung vị) là giá trị ở giữa của 1 tập hợp dữ liệu đã sắp xếp theo thứ tự. Trong trường hợp số phần tự của tập hợp là số chẵn, ta lấy trung bình cộng của 2 giá trị ở giữa làm median, tức là lấy \\((1/2)(y_{n/2}\\;+\\;y_{(n/2+1)})\\). Trong R, ta tính median bằng hàm median(): median(TenMileRace$net) ## [1] 5555 # Dùng dplyr summarise(TenMileRace, median(net)) ## median(net) ## 1 5555 4.2.3 Trimmed mean Giá trị “trimmed mean” là giá trị trung bình sau khi “xén” bớt một số phần trăm nhất định ở 2 đầu phân phối; ví dụ đối với 5% trimmed mean, sau khi sắp xếp dữ liệu theo thứ tự, ta lấy bớt 5% các quan sát đầu tiên và cuối cùng, giữ lại 90% rồi lấy trung bình cộng. Trong R, ta tính trimmed mean bằng hàm mean() kèm theo thông số trim = n: mean(TenMileRace$net, trim=0.5) ## [1] 5555 Có thể thấy, sau khi xén bớt 5% quan sát ở 2 đầu phân phối, mean và median của trường net trong dataset TenMileRace bằng nhau. 4.2.4 Ví dụ Một lớp học có 9 người, tuổi của cả lớp là tập hợp \\(\\{21, 22, 23, 25,26,22, 23,21, 26\\}\\). Mean của tập hợp là 23.222, median là 23; hai giá trị này hiện đang xấp xỉ nhau. Người thứ 10 nhập học có tuổi là 71, khiến mean của tập hợp trở thành 28.6, median vẫn giữ nguyên là 24. Giá trị mean mới lớn hơn so với median, do chúng ta thêm vào 1 outlier lớn hơn hẳn. Dữ liệu lúc này được gọi là lệch sang bên phải (skewed). Đối với dữ liệu có độ lệch lớn, giá trị median sẽ thể hiện xu hướng trung tâm tốt hơn mean. age &lt;- c(21, 22, 23, 25,26,22, 23,21, 26) mean(age) ## [1] 23.22222 median(age) ## [1] 23 age &lt;- append(age, 71) # thành viên mới của lớp học mean(age) ## [1] 28 median(age) ## [1] 23 4.3 Mức độ phân tán (Spread/Dispersion) Câu hỏi thứ 2 khi làm việc với 1 dataset là “Mức độ phân tán của dataset này như thế nào?”. Ví dụ, ta có 2 tập dữ liệu có các tham số trung tâm tương đương nhau, nhưng không có nghĩa là 2 tập này giống nhau, mà dữ liệu có thể phân tán và biến thiên khác nhau. Có nhiều cách để trả lời câu hỏi này: 4.3.1 Range (Khoảng biến thiên) Range (khoảng biến thiên) là khoảng cách giữa giá trị quan sát nhỏ nhất đến giá trị quan sát lớn nhất. \\[Range=Max-Min\\] TenMileRace %&gt;% summarise(range = max(net) - min(net)) ## range ## 1 7722 4.3.2 Inter-Quartile Range Interquartile range là khoảng cách giữa tứ phân vị thứ 3 (3rd quartile) và thứ phân vị thứ nhất (1st quartile): \\[IQR=Q3-Q1\\] Trước tiên, cần giải thích định nghĩa của các thuật ngữ -tile: Percentile: Tiếng Việt nghĩa là “bách phân vị”; sau khi xếp dữ liệu theo thứ tự, giá trị phần trăm thứ \\(p\\) (\\(p_{th}\\) percentile) là giá trị quan sát có \\(p\\) phần trăm số quan sát bên dưới và \\((1-p)\\) bên trên, trong đó \\(p\\) nằm giữa 0 và 100. Percentile thứ 0 và 100 là chính là giá trị nhỏ nhất và lớn nhất của tập dữ liệu. Giá trị median là giá trị phần trăm thứ 50 (50th percentile). Percentile là một mốc, không phải 1 khoảng dữ liệu. Quartile: Thông thường, chúng ta sẽ chia dữ liệu ra làm 4 phần với 3 mốc 25th percentile, 50th percentile và 75th percentile, tức là chia dữ liệu ra làm 4 phần. Ba mốc này cũng được gọi là 1st, 2nd và 3rd quartiles (khoảng phần tư thứ nhất, thứ 2 và thứ 3). Quantile: Là các điểm cắt/điểm mốc chia một khoảng phân phối xác suất ra thành các khoảng với xác suất giống nhau. Như vậy, quartile và percentile là các trường hợp đặc biệt của quantile. Dùng hàm quantile(): quantile(TenMileRace$net) ## 0% 25% 50% 75% 100% ## 2814 4950 5555 6169 10536 # so sánh giá trị 0%, 50% và 100% # với các hàm min, max và median c(min(TenMileRace$net), median(TenMileRace$net), max(TenMileRace$net)) ## [1] 2814 5555 10536 Chúng ta có thể dùng argument probs trong hàm quantile() để tìm ra các percentile khác: quantile(TenMileRace$net, probs = c(0.05, 0.95)) ## 5% 95% ## 4098.75 7260.50 Giá trị mặc định của argument probs là 0 (min), 0.25 (1st quartile), 0.5 (2nd quartile/median), 0.75 (third quartile) và 1 (max). Để tính IQR, chúng ta hàm IQR (nhớ viết hoa): IQR(TenMileRace$net) ## [1] 1219 # hoặc kết hợp với dplyr TenMileRace %&gt;% summarise(IQR(net)) ## IQR(net) ## 1 1219 Lưu ý: hàm quantile mặc định trả về 5 giá trị, vì vậy sẽ không kết hợp được với dplyr::summarise do hàm summarise chỉ trả về 1 kết quả. Nếu thay đổi argument probs thì có thể kết hợp được. TenMileRace %&gt;% summarise(quantile(net, probs = 0.5)) ## quantile(net, probs = 0.5) ## 1 5555 4.3.3 Variance (Phương sai) Trước tiên, ta xem xét khoảng cách từ một quan sát bất kì đến mean (deviation). Ta định nghĩa deviation thứ \\(i_{th}\\) là: \\[e_{i}=x_{i}-\\bar{x}\\] Vậy khoảng cách trung bình đến mean là bao nhiêu? Nếu khảo sát mức độ phân tán bằng cách này, ta gặp phải 2 vấn đề: Tổng của tất cả các khoảng cách từ các quan sát đến mean bằng 0: \\[\\sum_{i=1}^{n}(x_{i}-\\bar{x}) = \\sum_{i=1}^{n}x_{i}-\\sum_{i=1}^{n}\\bar{x} = n\\frac{1}{n}\\sum_{i=1}^{n}x_{i}-n\\bar{x} = n\\bar{x}-n\\bar{x} = 0\\] Vấn đề lớn hơn: Các giá trị khoảng cách đến mean có cả giá trị âm và dương, và khi khảo sát mức độ phân tán, kết quả âm hay dương là vô nghĩa. Để giải quyết vấn đề này, chúng ta có thể lấy giá trị tuyệt đối hoặc bình phương kết quả. Nếu đi theo hướng lấy giá trị tuyệt đối, ta dùng chỉ số MAD (Mean Absolute Deviation), được tính là trung bình của tổng giá trị tuyệt đối của khoảng cách từ các observation đến mean: \\((1/n)\\sum_{i=1}^{n}|x_{i}-\\bar{x}|\\). Nếu đi theo hướng bình phương kết quả, ta dùng phương sai (variance). Trong thực tế, dữ liệu thường ở dạng một phân phối chuẩn (normal distribution - sẽ nhắc đến ở phần 2), như chiều cao con người, huyết áp, điểm thi, giá cổ phiếu, v.v. Phân phối chuẩn được định nghĩa bởi 2 thông số là mean và variance, và vì mức độ quan trọng của phân phối chuẩn, nên variance được sử dụng nhiều hơn khi tính toán mức độ phân tán. Sample variance (Phương sai mẫu), với \\(\\bar{x}\\) là giá trị trung bình cộng của mẫu, \\(n\\) là số observation: \\[s^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2\\] Population variance (Phương sai quần thể), với \\(\\mu\\) là giá trị trung bình cộng của quần thể, \\(n\\) là số observation: \\[\\sigma^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\mu)^2\\] Dữ liệu thu thập được thường ở dạng một mẫu của cả quần thể. Nếu trong trường hợp có dữ liệu của cả population, ta sẽ dùng công thức 2. Ở cả 2 công thức, ta lấy tổng bình phương của khoảng cách tới mean, sau đó lấy trung bình, tuy nhiên ta chia cho \\((n-1)\\) để tính sample variance (phương sai mẫu) thay vì chia cho \\(n\\) để tính population variance (phương sai quần thể). Ta làm điều này để tránh việc (\\(\\sigma^{2}\\)) bị “biased” - có nghĩa là nếu dùng công thức tính population variance cho một sample, kết quả tính ra thường nhỏ hơn bình thường (do ta đang tính \\(s^2\\), vốn là 1 chỉ số ước lượng (estimator), dựa trên 1 chỉ số ước lượng khác là \\(\\bar{x}\\)). Để tính sample variance trong R, chúng ta sử dụng hàm var(): var(TenMileRace$net) ## [1] 940233.5 4.3.4 Standard variation (độ lệch chuẩn) Vấn đề lớn nhất của sample variance là đơn vị ở dạng bình phương, dẫn đến việc gây khó hiểu, khó để dẫn đến kết luận/phân tích. Để giải quyết, ta lấy giá trị căn bậc 2 để có được độ lệch chuẩn mẫu (sample standard deviation). \\[s=\\sqrt{s^2}\\] Để tính độ lệch chuẩn mẫu trong R, ta dùng hàm sd(): sd(TenMileRace$net) ## [1] 969.6564 Về mặt toán học, phương sai quan trọng hơn vì nó được sử dụng làm định nghĩa cho các phân phối xác suất, nhưng độ lệch chuẩn dễ hiểu hơn và vì thế có ích trong thực tế hơn. 4.3.5 Hệ số biến thiên (Coefficient of variation) Chúng ta xem xét ví dụ sau: Có 1 nhóm động vật có độ lệch chuẩn mẫu của chiều dài con vật là 15cm. Chỉ với thông tin này, ta gần như không thể đưa ra kết luận gì có ích. Giả sử có thêm thông tin về loài động vật: Nếu là voi: ta có thể kết luận nhóm các con voi này có chiều dài rất đồng đều, do chiều dài trung bình của cả loài voi nói chung sẽ lớn hơn 15cm rất nhiều, và ta có thể dựa vào common sense để nhận định rằng chiều dài giữa các con voi trong nhóm lệch nhau rất nhỏ. Nếu là côn trùng: ta có thể kết luận nhóm côn trùng này có chiều dài rất đa dạng, thậm chí là có nhiều loài côn trùng khác nhau. Trong thực tế, loài côn trùng dài nhất cũng có chỉ độ dài khoảng 60cm. Như vậy, nếu có thêm thông tin về loài động vật, hay nói cách khác là thông tin giúp ta ước lượng được chiều dài trung bình của nhóm động vật trên, ta sẽ đưa ra được nhiều kết luận có ích. Để giải quyết vấn đề này, ta có thể dùng hệ số biến thiên: \\[CV = \\frac{s}{|\\bar{x}|}\\] 4.3.6 Quy tắc thực nghiệm Ta có thể sử dụng quy tắc thực nghiệm như sau để áp dụng cho các mẫu dữ liệu tương đối lớn và đối xứng: Khoảng Số lượng phần trăm quan sát (tương đối) \\(\\bar{x}\\pm s\\) 68.3% \\(\\bar{x}\\pm 2s\\) 95.5% \\(\\bar{x}\\pm 3s\\) 99.7% 4.4 Tiếp tục với biểu đồ 4.4.1 Box plot 4.4.2 Scatter plot 4.4.3 Skewness &amp; Kurtosis 4.5 Tổng hợp thuật ngữ 4.6 Một số bài tập 4.7 Tham khảo "],
["xác-su-t-c-b-n.html", "5 Xác suất cơ bản 5.1 Một số định nghĩa cơ bản 5.2 Lý thuyết tập hợp 5.3 Các phép toán/quan hệ tập hợp 5.4 Quy luật xác suất 5.5 Biến ngẫu nhiên rời rạc 5.6 Các phân phối rời rạc phổ biến 5.7 Biến ngẫu nhiên liên tục 5.8 Các phân phối liên tục phổ biến", " 5 Xác suất cơ bản # load sẵn ggplot2 (đã bao gồm trong tidyverse) library(tidyverse, quietly = TRUE) library(venn) 5.1 Một số định nghĩa cơ bản Probability: Likelihood, relative frequency, chance. Outcome: Kết quả/biến số -&gt; Là một quan sát (observation) của một thí nghiệm (experiment) ngẫu nhiên, ví dụ: một lần tung xúc xắc. Trong mục này ta sẽ giữ nguyên từ “outcome”. Experiment: quá trình dẫn đến một hoặc nhiều outcome. Sample space: Không gian mẫu -&gt; tập hợp tất cả các outcome của thí nghiệm, còn được gọi là “universal set”. Từ này chúng ta sẽ cố gắng sử dụng tiếng Việt. Event: Sự kiện -&gt; là tập hợp các outcome với xác suất nhất định. Ví dụ: Lăn xúc xắc ra số nhỏ hơn 3. Có 3 cách hiểu về xác suất: clasical, relative frequency và subjective. 5.2 Lý thuyết tập hợp 5.2.1 Biểu đồ Venn library(VennDiagram, quietly = TRUE) Đặt \\(S\\) là tập hợp tất cả các outcome của thí nghiệm ngẫu nhiên (tức là sample space). Trong đó, chúng ta quan tâm đến hai sự kiện \\(A\\) và \\(B\\). Sử dụng biểu đồ Venn để thể hiện các sự kiện, với thí nghiệm là lăn một lần xúc xắc có 6 mặt, tức là \\(S = \\{1,2,3,4,5,6\\}\\), với: \\(A\\) là “lăn xúc xắc ra số lẻ” \\(B\\) là “lăn xúc xắc ra lớn hơn 5” 5.3 Các phép toán/quan hệ tập hợp 5.3.1 Phép hợp (Union) Phép hợp thể hiện hoặc A và B sẽ sảy ra (quan hệ either or), được ký hiệu là \\(A\\cup B\\). 5.3.2 Phép giao (intersection) Phép giao thể hiện sự kiện cả A và B cùng xảy ra (both), ký hiệu là \\(A\\cap B\\). 5.3.3 Phần bù (Complement) Phần bù tương đối, hay còn gọi là phép hiệu (difference), ký hiệu là \\(A \\setminus B\\). Đây là tập hợp các phần tử có trong A nhưng không có trong B. Phần bù tuyệt đối (absolute complement) Phần bù tuyệt đối của tập hợp A để thể hiện sự kiện “A không xảy ra”, tương ứng với tất cả các outcome không thuộc A, có thể nằm trong một không gian mẫu toàn thể khác. 5.3.4 Loại trừ lẫn nhau (Mutually exclusive) Quan hệ loại trừ lẫn nhau (hay còn gọi là xung khắc) - mutually exclusiveness/disjoint - ký hiệu là \\(\\bar{A}\\) hoặc \\(A^{C}\\). Định nghĩa 1. Hai sự kiện A và B được gọi là loại trừ lẫn nhau nếu sự xảy ra của sự kiện này ngăn việc xảy ra của sự kiện kia. Ví dụ: Trong 1 lần đổ xúc xắc, số 2 và số 5 không thể cùng xảy ra. 5.4 Quy luật xác suất 5.4.1 Các quy luật cơ bản Trong phần này, ta sử dụng biểu đồ Venn, cụ thể là phần diện tích trong biểu đồ, để thể hiện xác suất của một sự kiện. Định nghĩa 2. Xác suất là tỷ lệ số lần một sự kiện xảy ra trong nhiều lần thử lặp lại của một hiện tượng ngẫu nhiên. Nói một cách khác, xác suất chính là tần suất tương đối (relative frequency) về lâu dài. Quy luật 1. Với bất kỳ sự kiện \\(A\\), xác suất \\(P(A)\\) luôn thỏa mãn \\(0\\le P(A) \\le 1\\) vì tỷ lệ luôn nằm trong khoảng \\([0,1]\\). Quy luật 2. Nếu hai sự kiện loại trừ lẫn nhau, \\(PA \\cup B = P(A) + P(B)\\). Vì S là tập hợp tất cả các sự kiện có thể xảy ra, diện tích của hình chữ nhật bên ngoài biểu đồ sẽ là 1 và xác suất của sự kiện A được thể hiện bằng diện tích hình tròn A. Khi A và B loại trừ lẫn nhau, A và B là hai hình tròn tách biệt. 5.4.2 Xác suất có điều kiện (conditional probability) Ở phần trước, chúng ta xem xét các mối quan hệ tập hợp, sử dụng để thể hiện sự liên quan của 2 sự kiện A và B. Tuy nhiên, với phép hợp hay phép giao, A và B không có mối quan hệ phụ thuộc. Ở phần này, ta xem xét tiếp tình huống có từ hai sự kiện trở lên xảy ra, và một sự kiện diễn ra trước sự kiện còn lại, tức là trở thành điều kiện của sự kiện còn lại. Xác suất của A xảy ra nếu B đã xảy ra được viết là \\(P(A|B)\\), còn có thể gọi là “xác suất của A với điều kiện B”. Ví dụ 1: Để hình dung rõ ràng hơn về xác suất có điều kiện, ta xét một ví dụ cực kì đơn giản: Nếu tung hai đồng xu cùng một lúc, ta có 4 outcome là \\(\\{hh, ht, th, tt\\}\\) (h là head, t là tail). Như vậy xác suất tung cả hai đồng xu ra mặt ngửa là 25% (\\(1/4\\)). Thay vì tung cùng một lúc, ta tung lần lượt. Nếu đồng xu thứ nhất ra mặt ngửa, thì xác suất để cả hai đều ngửa sau khi tung nốt đồng xu thứ hai là bao nhiêu? Nếu nghĩ một cách đơn giản theo common sense, ta thấy nếu bây giờ chỉ còn một đồng xu để tung thì chỉ còn hai kết quả hoặc sấp hoặc ngửa. Vậy rõ ràng xác suất là 50%? Nếu để ý kỹ hơn, ta thấy không gian mẫu ban đầu bị thu nhỏ lại và chỉ còn hai khả năng xảy ra: \\(\\{hh, ht\\}\\), tức là \\(P(H | H) = 1/2\\). Ví dụ 2: trò chơi roullette truyền thống có 37 ô, tất cả đều như nhau (xác suất như nhau = 1/37). Có 18 ô đỏ, 18 ô đen, kèm với 1 ô số 0. Trong số các ô đỏ có 8 ô số chẵn (và 10 lẻ); trong số các ô đen có 10 ô số sẵn (và 8 lẻ). Tình huống: Nếu vòng roullette được xoay bí mật, và sau đó bạn được cho biết là kết quả rơi vào ô màu đỏ -&gt; tính xác suất kết quả cũng là số lẻ? Trước tiên cần định nghĩa 2 sự kiện A và B: Sự kiện A: kết quả ra là ô màu đỏ Sự kiện B: kết quả ra là số lẻ Hai sự kiện này có mối quan hệ là: A xảy ra trước, và xác suất của B phụ thuộc vào A. Tức là sẽ khác với việc A và B xảy ra cùng lúc (intersection). Với tình huống xác suất có điều kiện như thế này, không gian mẫu của B sẽ phụ thuộc vào A, và không còn là không gian mẫu ban đầu nữa (37 ô). Nếu kết quả là một ô màu đỏ, và có 10 trong số 18 ô đỏ là số lẻ, thì \\(P(odd|red) = 10/18 = 5/9\\). Có thể thấy tử số là 18, thay vì là 37, vì ta đang xét trên không gian mẫu nhỏ hơn là tập các ô có màu đỏ. So sánh với 2 trường hợp: \\(P(odd) = 18/37\\), tức là không gian mẫu có 37 outcome. \\(P(odd giao red) = 10/37\\), tức là không gian mẫu vẫn có 37 outcome. Trường hợp này, có thể hiểu là ta tính xác suất của sự kiện kết quả xảy ra vừa có màu đỏ, vừa là số lẻ, mà không có điều kiện nào đi trước. Ví dụ 3: Chúng ta xét tiếp một ví dụ tương tự, nhưng sử dụng một bảng xác suất để dễ hình dung hơn: \\(\\,\\) Fire Auto Other Fraudulant 6% 1% 3% non-Fraudulant 14% 29% 47% Trên đây là dữ liệu về các case đòi bồi thường bảo hiểm và tỷ lệ lừa đảo, lấy từ sách “An Introduction to Statistical Methods &amp; Data Analysis” (Ott &amp; Longnecker). Ví dụ trên với vòng xoay roullette có không gian mẫu toàn thể là \\(P(Color \\cap Parity)\\) (màu sắc giao với tính chẵn lẻ). Ở ví dụ này, dữ liệu chính là \\(P(Category \\cap PolicyType)\\). Cộng dồn các cột và các hàng, ta tìm ra được xác suất của từng hạng mục (category) và từng loại hợp đồng bảo hiểm (policy). \\(\\,\\) Fire Auto Other \\(\\,\\) Fraudulant 6% 1% 3% 10% non-Fraudulant 14% 29% 47% 90% \\(\\,\\) 20% 30% 50% 100% Có thể thấy các case đòi bổi thường do cháy nổ dễ có khả năng lừa đảo hơn các case loại khác. Ta tính được tỷ lệ số case lừa đảo liên quan đến cháy nổ là: \\[\\begin{aligned} P(\\textrm{ Fraud }|\\textrm{ FirePolicy }) &amp;= \\frac{\\textrm{proportion of claims that are fire policies and are fraudulent}}{\\textrm{proportion of fire claims}} \\\\ &amp;= \\frac{6\\%}{20\\%}\\\\ &amp; \\\\ &amp;= 0.3 \\end{aligned}\\] Như vậy, có thể định nghĩa xác suất có điều kiện (với giả thiết \\(P(B) \\ne 0\\)) là: \\[P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\] Công thức trên có thể viết lại thành: \\[\\begin{aligned} P(A\\cap B) &amp;= P(A\\,|\\,B)\\,P(B) \\\\ &amp;= P(B\\,|\\,A)\\,P(A) \\end{aligned}\\] Vì thứ tự không quan trọng và \\(P\\left(A\\cap B\\right)=P\\left(B\\cap A\\right)\\). Sử dụng quy luật này, ta tính được xác suất một case đòi bổi thường là hợp đồng Auto với điều kiện case này không lừa đảo: \\[\\begin{aligned} P\\left(\\,Auto\\;|\\;NotFraud\\,\\right) &amp;= \\frac{P\\left(\\,Auto\\;\\cap\\;NotFraud\\right)}{P\\left(\\,NotFraud\\,\\right)} \\\\ &amp;= \\frac{0.29}{0.9} \\\\ &amp; \\\\ &amp;= 0.3\\bar{2} \\end{aligned}\\] Ví dụ 4: Giả sử bạn là một nhà nghiên cứu thị trường về điện thoại, và bạn thu thập được dử liệu (giả định, không phải dữ liệu thật) với các thông tin về hãng (Apple vs Samsung) và giới tính người dùng (nam vs nữ). Trong dữ liệu, điện thoại Apple chiếm 40% thị phần. Điều này có nghĩa là \\(P(Apple) = 0.40\\). Trong số điện thoại Apple, số người sử dụng là nam chỉ chiếm 35%, tức là \\(P(Men|Apple) = 0.35\\). Tuy nhiên, trong số người dùng Samsung, số lượng nam lại nhiều hơn, vì vậy \\(P(Men|Samsung) = 0.60\\). Với các thông tin trên, ta có bảng sau: \\(\\,\\) Samsung Apple \\(\\,\\) Nam Nữ \\(\\,\\) 60% 40% 100% Có thể điền thêm các thông tin còn thiếu trong bảng bằng cách sử dụng định nghĩa xác suất có điều kiện. Ví dụ: \\[\\begin{aligned} P\\left(Men\\,\\cap\\,Samsung\\right) &amp;= P\\left(Men\\,|\\,Samsung\\right)\\,P\\left(Samsung\\right) \\\\ &amp;= 0.60*0.60 \\\\ &amp;= 0.36 \\end{aligned}\\] Như vậy số lượng điện thoại là Samsung có người dùng nam là 0.36%. Tương tự, ta tính được: \\[\\begin{aligned} P\\left(Men\\,\\cap\\,Apple\\right) &amp;= P\\left(Men\\,|\\,Apple\\right)\\,P\\left(Apple\\right) \\\\ &amp;= 0.35*0.40 \\\\ &amp;= 0.14 \\end{aligned}\\] Từ đó, có bảng hoàn chỉnh như sau: \\(\\,\\) Samsung Apple \\(\\,\\) Nam 36% 14% 50% Nữ 24% 26% 50% \\(\\,\\) 60% 40% 100% Tóm lại, các con số 36% hay 14% là được dựa trên không gian dữ liệu toàn thể, bao gồm toàn bộ số lượng điện thoại được sử dụng trong phân tích. Nếu chúng ta giới hạn không gian này trong số điện thoại Samsung hay Apple, các con số trên sẽ thay đổi (thành 60% và 35%). 5.4.2.1 Tính độc lâp Hai sự kiện ngoài quan hệ phụ thuộc vào nhau còn có thể độc lập hoàn toàn với nhau. Ta có định nghĩa: Định nghĩa 3. Hai sự kiện A và B được coi là độc lập nếu \\(P(A|B) = P(A)\\) và \\(P(B|A) = P(B)\\). Hiểu theo cách khác, việc biết các outcome của sự kiện A không cho ta thông tin gì về các coutcome của sự kiện B, tức là xác suất của A giữ nguyên cho dù B có xảy ra hay không. Với phương pháp lấy mẫu ngẫu nhiên cơ bản (random sampling), ta thường giả định rằng hai biến bất kỳ luôn độc lập. Quy luật 3.. Nếu A và B là hai sự kiện độc lập, thì \\(P(A /cap B) = P(A|B)P(B) = P(A)P(B)\\). 5.4.3 Tổng hợp quy luật xác suất \\[0 \\le P\\left(A\\right) \\le 1\\] \\[P\\left(A\\right)+P\\left(\\bar{A}\\right)=1\\] \\[P\\left(A\\cup B\\right) = P\\left(A\\right)+P\\left(B\\right)-P\\left(A\\cap B\\right)\\] \\[P\\left(A\\cap B\\right) = \\begin{cases} P\\left(A\\,|\\,B\\right)P\\left(B\\right)\\\\ P\\left(B\\,|\\,A\\right)P\\left(A\\right)\\\\ P(A)P(B)\\;\\; &amp; \\textrm{ nếu A, B độc lập} \\end{cases}\\] \\[P\\left(A\\,|\\,B\\right) = \\frac{P\\left(A\\cap B\\right)}{P\\left(B\\right)}\\] 5.5 Biến ngẫu nhiên rời rạc Trong thực tế, lý thuyết xác suất được sử dụng để tính toán khả năng xảy ra của nhiều loại sự kiện khác nhau, bao gồm các sự kiện mang tính định lượng (quantitative) và định tính (qualitative). Ví dụ: với các sự kiện mà kết quả thu về không đo đếm được, dữ liệu có thể là các câu trả lời “Đồng ý” hoặc “Không đồng ý” thu được từ một bản survey, và được xếp loại vào một biến định tính. Mặt khác, các dữ liệu về thời gian, trọng lượng, etc. thường được xếp loại vào một biến định lượng. Chúng ta có thể đưa ra định nghĩa về hai loại biến ngẫu nhiên này: Biến ngẫu nhiên liên tục (random continuous variable - biến định lượng): là biến bao gốm các giá trị có dạng số (numeric) và, trên lý thuyết, có thể nhận một số lượng giá trị không đếm được. Nói cách khác, nêu giá trị số có thể diễn đạt hoặc biến đổi theo kiểu số thập phân hoặc phân số, giá trị này thường là kiểu liên tục. Trong R, dữ liệu liên tục thường là kiểu numeric. Biến ngẫu nhiên rời rạc (random discrete variable - định tính): là biến nhận một tập hợp các giá trị có thể đếm được. Nói cách khác, nếu việc diễn đạt giá trị ở dạng phân số hoặc thập phân trở nên vô nghĩa, giá trị thường sẽ là kiểu rời rạc. Nêu các quan sát của một biến biến định lượng chỉ có thể nhận một số các giá trị nhất định, biến này cũng được coi lả rời rạc. Trong R, dữ liệu rời rạc thường là kiểu factor hoặc string. 5.5.1 Giới thiệu về biến ngẫu nhiên rời rạc (NNRR) Các quy luật sau áp dụng với biến NNRR: Xác suất của mỗi giá trị nằm giữa khoảng 0 và 1 Tổng xác suất của tất cả các giá trị bằng 1 Xác xuất của các biến NNRR có thể cộng với nhau, tức là \\(P(3 \\ hoặc \\ 4) = P(3) + P(4)\\) 5.5.1.1 Giá trị kỳ vọng Ví dụ: Xét biến NNRR \\(S\\) là tổng kết quả của sự kiện thả 2 xúc xắc. Ta có biểu đồ thể hiện xác suất của các giá trị mà \\(S\\) CÓ THỂ NHẬN: Xác suất thường được áp dụng để đưa ra các phỏng đoán về tương lai, và một khái niệm cơ bản được xử dụng là giá trị kỳ vọng (expected value) Trong trường hợp tung xúc xắc, ta có giá trị kỳ vọng \\(E[S]\\) chính là mean của tập hợp \\(D\\) gồm tất cả các giá trị mà \\(S\\) có thể nhận. Ta biết trước được mọi giá trị mà \\(S\\) có thể nhận vì tập hợp này không lớn và dễ dàng suy ra được. Trong thực tế, việc có toàn bộ population là hiếm khi xảy ra, nên ta thường lấy một sample rất lớn và tìm giá trị kỳ vọng bằng cách tính sample mean, hay nói cách khác là weighted average của các giá trị có thể nhận được, với trọng số là xác suất (hay frequency) của từng giá trị. \\[\\mu=E[S] = \\sum_{\\textrm{possible }s}\\;s\\cdot P\\left(S=s\\right)\\] Trong ví dụ trên, ta có: \\[\\begin{aligned} \\mu = E[S] &amp;= \\sum_{s=2}^{12}s\\cdot P(S=s) \\\\ &amp;= 2\\cdot P\\left(S=2\\right)+3\\cdot P\\left(S=3\\right)+\\dots+11\\cdot P\\left(S=11\\right)+12\\cdot P\\left(S=12\\right) \\\\ &amp;= 2\\left(\\frac{1}{36}\\right)+3\\left(\\frac{2}{36}\\right)+\\dots+11\\left(\\frac{2}{36}\\right)+12\\left(\\frac{1}{36}\\right) \\\\ &amp;= 7 \\end{aligned}\\] Sau khi đã biết \\(E[S]\\) trong ví dụ trên, ta tiếp tục thí nghiệm bằng cách tung 2 xúc sắc 100 lần: x &lt;- sample(seq(2,12, by=2), size = 100, prob = rep(1/6, 6), replace = TRUE) mean(x) ## [1] 6.8 Có thể thấy trong thí nghiệm này, giá trị trung bình không bằng với \\(E[S]\\). Nếu lặp lại thí nghiệm này với số lần \\(n\\) lớn hơn thì khi \\(n\\) tiến tới vô hạn, giá trị trung bình sẽ tiến tới giá trị kỳ vọng. 5.5.1.2 Phương sai Tương tự, ta định nghĩa phương sai (variance) của S (ký hiệu là \\(\\sigma^{2}\\)) là weighted average của bình phương độ lệch chuẩn (squared-deviation) có thể xảy ra: \\[ \\sigma^{2}=V[S] = \\sum_{\\textrm{possible }s}\\; (s-\\mu)^2 \\cdot P\\left(S=s\\right)\\] Trong ví dụ trên, \\(\\sigma^{2}\\) bằng: \\[\\begin{aligned} \\sigma^{2}=V[S] &amp;= \\sum_{s=2}^{12}\\left(s-\\mu\\right)^{2}P(S=s) \\\\ &amp;= (2-7)^{2}\\left(\\frac{1}{36}\\right)+(3-7)^{2}\\left(\\frac{2}{36}\\right)+\\dots+(12-7)^{2}\\left(\\frac{1}{36}\\right) \\\\ &amp;= \\frac{35}{6}=5.8\\bar{3} \\end{aligned}\\] Phương sai và giá trị kỳ vọng là 2 đại lượng quan trọng để định nghĩa và miêu tả một phân phối xác suất. (???) 5.6 Các phân phối rời rạc phổ biến 5.6.1 Binomial distribution - PP nhị thức 5.6.1.1 Định nghĩa Ví dụ: Giả sử có ba chiếc bẫy được đặt tại ba vị trí trong rừng. Ba vị trí này xa nhau đủ để không ảnh hưởng đến xác suất của các bẫy khác bắt được thú. Ta có: Sự kiện bẫy bắt được thú là \\(C_i\\) Sự kiện bẫy trống là \\(E_i\\) Xác suất bẫy bắt được thú là \\(\\pi = 0.8\\) Các kết quả có thể nhận được là: Outcome \\(\\,\\) \\(E_1, E_2, E_3\\) \\(\\,\\) \\(C_1, E_2, E_3\\) \\(\\,\\) \\(E_1, C_2, E_3\\) \\(\\,\\) \\(E_1, E_2, C_3\\) \\(\\,\\) \\(C_1, C_2, E_3\\) \\(\\,\\) \\(C_1, E_2, C_3\\) \\(\\,\\) \\(E_1, C_2, C_3\\) \\(\\,\\) \\(C_1, C_2, C_3\\) \\(\\,\\) Vì các bẫy ở xa nhau nên kết quả của từng bẫy là độc lập với nhau. Ta có: \\[ P(E_1 \\cap E_2 \\cap E_3) = P(E_1)P(E_2)P(E_3) = (1 - 0.8)0.8(1-0.8) = 0.032\\] Tương tự, ta tính được xác suất của các kết quả còn lại trong bảng: Outcome Probability \\(S\\) Outcome Probability \\(E_1, E_2, E_3\\) 0.008 \\(S=0\\) 0.008 ——————- ————— ————- ————— \\(C_1, E_2, E_3\\) 0.032 \\(E_1, C_2, E_3\\) 0.032 \\(S=1\\) \\(3(0.032) = 0.096\\) \\(E_1, E_2, C_3\\) 0.032 ——————- ————— ————- ————— \\(C_1, C_2, E_3\\) 0.128 \\(C_1, E_2, C_3\\) 0.128 \\(S=2\\) \\(3(0.128) = 0.384\\) \\(E_1, C_2, C_3\\) 0.128 ——————- ————— ————- ————— \\(C_1, C_2, C_3\\) 0.512 \\(S=3\\) \\(0.512\\) Tiếp đó, ta xét đến biến ngẫu nhiên \\(S\\) và tổng hợp được xác suất của các giá trị mà \\(S\\) nhận được, tương đương với số bẫy bắt được mồi: \\(S\\) Outcome Probability \\(S=0\\) \\(0.008=0.2^3\\cdot0.8^0\\) \\(S=1\\) \\(0.096=3\\cdot0.2^2\\cdot0.8^1\\) \\(S=2\\) \\(0.384=3\\cdot0.2\\cdot0.8^2\\) \\(S=3\\) \\(0.512=0.2^0\\cdot0.8^3\\) Do các sự kiện có tính độc lập với nhau nên tổng xác suất của mọi sự kiện bằng \\(0.008 + 0.096 + 0.384 + 0.512 = 1\\). Đồng thời, ta cũng nhận thấy rằng: Ở mỗi bẫy, xác suất để bẫy có bất kỳ outcome nào là \\(\\pi + 1 - \\pi = 1\\) Do có 3 bẫy, nên tổng xác suất là: \\[\\begin{aligned} (\\pi + (1-\\pi))^3 &amp;= \\pi^3 + 3\\pi(1-\\pi)^2 + 3\\pi^2(1-\\pi) + (1-\\pi)^3 \\\\ &amp;= 1 \\end{aligned}\\] Tức là trong trường hợp tổng quát với \\(n\\) lần thử, ta sử dụng định lý nhị thức, trong đó mỗi phần tử tương đương với xác suất mà biến \\(S\\) nhận giá trị \\(k\\) - \\(P(S=k)\\): \\[\\begin{aligned} (\\pi + (1-\\pi))^n &amp;= {n \\choose 0}\\pi^n + {n \\choose 1}\\pi^{n-1}(1-\\pi)^1 + ... + {n \\choose n-1}\\pi^1(1-\\pi)^{n-1} + (1-\\pi)^n \\\\ &amp;= \\sum_{k=0}^{n}{n \\choose k}\\pi^{k}(1-\\pi)^{n-k} \\end{aligned}\\] Như vậy, để tính \\(P(S=k)\\), ta chỉ cần sử dụng công thức (hay còn gọi là hàm khối xác suất - probability mass function - pmf) sau: \\[P(S=k)=\\underbrace{\\frac{n!}{x!(n-k)!}}_{hệ\\ số\\ nhị\\ thức}\\;\\underbrace{\\pi^{x}}_{k\\,thành\\ công}\\;\\underbrace{(1-\\pi)^{n-x}}_{n-k\\,thất\\ bại}\\] Hệ số nhị thức chính là số tổ hợp chập \\(k\\) của \\(n\\) phần tử, có thể hiểu là: có \\(\\frac{n!}{x!(n-k)!}\\) cách phân phối \\(k\\) lần thành công và \\(n-k\\) lần thất bại trong \\(n\\) lần thử. Như vậy, phân phối nhị thức là phân phối đáp ứng các điều kiện sau: Thí nghiệm bao gồm \\(n\\) lần thử giống nhau Mỗi lần thử có thể có 2 kết quả: thành công hoặc thất bại/đúng hoặc sai. Xác suất thành công trong mỗi phép thử là \\(\\pi\\) là giữ nguyên giữa các phép thử Các phép thử có tính độc lâp với nhau Tồn tại một biến ngẫu nhiên \\(S\\) là số lần thành công trong \\(n\\) phép thử Trong R, ta sử dụng hàm dbinom để mô phỏng lại phân phối nhị thức: dist &lt;- data.frame(x=0:3) %&gt;% mutate(xs=dbinom(x,size=3,prob=0.8)) ggplot(dist, aes(x=x)) + geom_point(aes(y=xs)) + geom_linerange(aes(ymax=xs, ymin=0)) + ggtitle(&#39;phân phối nhị phân: n=3, p=0.8&#39;) + theme_bw() Trong đồ thị trên, để tính chiều cao các đường tần suất, ta sử dụng hàm pmf tại điểm tương ứng. \\[\\begin{aligned} P(S=2) &amp;= {3 \\choose 2} (0.8)^2(1-0.8)^{3-2} \\\\ &amp;=\\frac{3!}{2!(3-2)!}(0.8)^2(0.2)^{3-2} \\\\ &amp;= 0.384 \\end{aligned}\\] Trong R, với mỗi phân phối xác suất, thường sẽ có 4 nhóm function với vai trò khác nhau: Nhóm d-function: Tính chiều cao của hàm pmf (hoặc pdf). Với biến NNRR \\(Y\\), d-function trả về kết quả tương đương với \\(P(Y=y)\\) với \\(y\\) bất kỳ. Nhóm p-function: Tính \\(P(Y\\le y)\\), tức là xác suất để \\(Y\\) nhận giá trị nhỏ hơn hoặc bằng \\(y\\). Nhóm q-function: Tìm quantile nhất định của phân phối. Ví dụ, giá trị nào chia phân phối tại các ngưỡng 25% và 75%? Nhóm r-function: Lấy một sample ngẫu nhiên từ phân phối. Trong hân phối nhị thức, 4 hàm tương ứng là dbinom, pbinom, qbinom và rbinom. 5.6.1.2 Giá trị kỳ vọng của phân phối nhị thức Xét ví dụ đơn giản sau: Nêu tung 100 đồng xu, với \\(X\\) là số lần ra mặt ngửa (head), ta có \\(E[X] = 50 = \\frac{1}{2}100\\) Hoặc: Một bài kiểm tra có 20 câu hỏi, mỗi câu 4 đáp án. Nếu chọn ngẫu nhiên cho cả 20 câu, số câu đúng kỳ vọng là \\(\\frac{1}{4}20 = 5\\). Tóm lại, có thể thấy với phân phối nhị thức, \\(E[X]=n\\pi\\). Ta chứng minh công thức này như sau: Ở phần trước, ta có với một phân phối xác suất nói chung, \\(E[X]\\) là mean của 1 sample đủ lớn, tức là \\(E[X] = \\sum\\;x\\cdot P\\left(X=x\\right)\\). Suy ra với phân phối nhị thức: \\[\\begin{aligned} E[X] &amp;= x \\cdot P\\left(X=x\\right) \\\\ &amp;=\\sum_{x=0}^{n}x{n \\choose x}\\pi^x(1-\\pi)^{n-x} \\\\ \\end{aligned}\\] Khi \\(x=0\\), phần tử đầu tiên trong tổng cũng bằng 0 nên: \\[\\begin{aligned} E[X] &amp;= x \\cdot P\\left(X=x\\right) \\\\ &amp;=\\sum_{x=0}^{n}x{n \\choose x}\\pi^x(1-\\pi)^{n-x} \\\\ &amp;=\\sum_{x=1}^{n}x{n \\choose x}\\pi^x(1-\\pi)^{n-x} \\\\ &amp;=\\sum_{x=1}^{n}x\\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} \\\\ &amp;=\\sum_{x=1}^{n}n\\frac{(n-1)!}{(x-1)!((n-1)-(x-1))!}\\pi^x(1-\\pi)^{n-x} \\\\ &amp;=n\\pi\\sum_{x=1}^{n}{n-1 \\choose x-1}\\pi^{x-1}(1-\\pi)^{(n-1)-(x-1)} \\\\ \\end{aligned}\\] Áp dụng định lý nhị thức: \\[\\begin{aligned} E[X] &amp;= \\vdots \\\\ &amp;=n\\pi(\\pi+(1-\\pi))^{n-1} \\\\ &amp;=n\\pi \\cdot 1^{n-1} \\\\ &amp;=n\\pi \\end{aligned}\\] Tương tự, phương sai được tính bằng: \\[\\begin{aligned} V[X] &amp;= \\sum_{x=0}^{n}\\left(x-E\\left[X\\right]\\right)^{2}\\,P\\left(X=x|n,\\pi\\right) \\\\ &amp;= \\sum_{x=0}^{n}\\left(x-E\\left[X\\right]\\right)^{2}\\;\\frac{n!}{x!\\left(n-x\\right)!}\\pi^{x}\\left(1-\\pi\\right)^{n-x} \\\\ &amp;= \\vdots \\\\ &amp;= n\\pi(1-\\pi) \\end{aligned}\\] 5.6.2 Poisson distribution - PP Poisson Phân phối Poisson được tạo ra với mục đích mô hình hóa số lượng sự kiện diễn ra trong một khoảng thời gian nhất định, hay mở rộng ra là trong một giới hạn cố định. Ví dụ: Số lượng khách hàng thanh toán mỗi tiếng số lượng hươu trên mỗi 1000 hecta số lượng búi tảo trong mỗi mét khối nước hồ Các dữ kiện trên là một kiểu tốc độ/tần suất thay đổi, trong PP Poisson ký hiệu là \\(\\lambda\\). Sử dụng phân phối Poisson, ta có thể dự đoán được xác suất mà số lượng sự kiện nào đó có thể xảy ra trong một giới hạn cố định. 5.6.2.1 So sánh với PP nhị thức Các ví dụ liệt kê ở trên có thể giải quyết được bằng cách sử dụng PP nhị thức, do ta đang tính xác suất xảy ra số sự kiện thành công (KH có thanh toán, hươu có tồn tại, etc.), gọi là \\(P(X)\\). Tuy nhiên, cần nhiều thông tin hơn để tính \\(P(X)\\), bao gồm xác suất thành công \\(\\pi\\) và tổng số lần thử \\(n\\). Trong khi đó, đề bài đưa ra lại chỉ có \\(\\lambda\\) là dữ kiện ở dạng tỷ lệ tăng/tốc độ: số KH thanh toán/tiếng, số hươu/hecta, etc, hay nói cách khác chính là giá trị kỳ vọng \\(E[X]\\). Như vậy, thách thức đầu tiên gặp phải là PP nhị thức cần nhiều dữ kiện hơn, trong khi Poisson chỉ cần 1 thông tin duy nhất. Tiếp đó, ta nhận thấy nếu tính được xác suất thành công trên mỗi đơn vị thời gian hoặc không gian, ta gặp phải vấn đề là PP nhị thức không chứa được NHIỀU HƠN MỘT sự kiện trong một đơn vị thời gian/không gian tương ứng. Có thể giải quyết vấn đề này bằng cách chia nhỏ đơn vị thời gian/không gian ra, ví dụ chia 1 tiếng thành 60 phút, hoặc thậm chí nhỏ hơn. Tuy nhiên, kết quả đưa ra sẽ luôn gặp giới hạn trong việc fit với dữ liệu thực tế. 5.6.2.2 Định nghĩa &amp; chứng minh PP Poisson Từ ví dụ trên, có thể thấy về mặt toán học, \\(n \\to \\infty\\), do càng chia nhỏ đại lượng thời gian/không gian, số sự kiện chứa được trong mỗi đơn vị càng tăng. Vì \\(E[X]\\) cố định nên \\(p \\to 0\\), do \\(E[X]= \\lambda = np\\). Như vậy, ta có thể suy ra công thức tính pmf của PP Poisson từ pmf của PP nhị thức như sau: \\[\\begin{aligned} P(Y=k) &amp;=\\lim_{n\\to\\infty}{n \\choose k} \\pi^k (1-\\pi)^{n-k} \\\\ &amp;=\\lim_{n\\to\\infty}{n \\choose k} \\left(\\frac{\\lambda}{n} \\right)^k \\left(1-\\frac{\\lambda}{n} \\right)^{n-k} \\\\ &amp;=\\lim_{n\\to\\infty} \\frac{n!}{(n-k)!k!} \\frac{1}{n^k} \\lambda^k \\left(1-\\frac{\\lambda}{n}\\right)^n \\left(1-\\frac{\\lambda}{n}\\right)^{-k} \\\\ &amp;= \\lim_{n\\to\\infty} \\underbrace{\\frac{n!}{(n-k)!} \\frac{1}{n^k}}_{=1} \\; \\underbrace{\\frac{\\lambda^k}{k!} \\left(1-\\frac{\\lambda}{n}\\right)^n}_{Poisson\\ PMF} \\; \\underbrace{\\left(1-\\frac{\\lambda}{n}\\right)^{-k}}_{=1} \\\\ &amp;= \\underbrace{\\frac{\\lambda^k}{k!} \\; e^{-\\lambda}}_{Poisson\\ PMF} \\end{aligned}\\] Phần chứng minh cụ thể cho từng thừa số nằm ở phụ lục cuối chương. Như vậy ta có: Với biến ngẫu nhiên \\(Y\\) nhận giá trị \\(y\\) là số sự kiện thành công trong một đơn vị thời gian/không gian nào đó, hay nói cách khác là hàm khối xác suất pmf là: \\[\\underbrace{\\frac{\\lambda^k}{k!} \\; e^{-\\lambda}}_{Poisson\\ PMF}\\] với \\(\\lambda\\) là số sự kiện kỳ vọng xảy ra trong một đơn vị thời gian/không gian và \\(e\\) là \\(2.718281828\\dots\\) (base of natural logarithm). Từ đó, ta suy ra: \\[E[Y] = \\lambda\\] \\[Var[Y] = \\lambda\\] Ví dụ: Có \\(Y\\) là số khách hàng thanh toán tại quầy của 1 cửa hàng tạp hóa trong vòng 12 tiếng. Giả sử \\(Y\\sim Poi(\\lambda=2.3)\\). Hỏi: Xác suất có 4 KH thanh toán trong 12 tiếng? \\[P(Y=4) = \\frac{2.3^4e^{-2.3}}{4!} = 0.1169\\] Xác suất có nhiều nhất 4 KH? \\[\\begin{aligned} P(Y\\le4) &amp;= P(Y=0)+P(Y=1)+P(Y=2)+P(Y=3)+P(Y=4) \\\\ &amp;= 0.1003+0.2306+0.2652+0.2033+0.1169 \\\\ &amp;= 0.9163 \\end{aligned}\\] Xác suất có nhiều hơn hoặc bằng 5 KH? \\[P(Y\\ge5)=1-P(Y\\le4) =1-0.9163 =0.0837\\] Các phép tính trên có thể thực hiện trên R với 4 nhóm hàm tương tự như PP nhị thức: dist &lt;- data.frame( NumCaught = 0:10 ) %&gt;% mutate( probability = dpois( NumCaught, lambda=2.3 ) ) ggplot(dist, aes(x=NumCaught)) + geom_point( aes(y=probability) ) + geom_linerange(aes( ymax=probability, ymin=0)) + ggtitle(expression(paste(&#39;Poisson Distribution with &#39;, lambda == 2.3))) + labs(x=&#39;Number Caught&#39;) + theme_bw() # P( Y = 4) dpois(4, lambda=2.3) ## [1] 0.1169022 # P( Y &lt;= 4) ppois(4, lambda=2.3) ## [1] 0.9162493 # 1-P(Y &lt;= 4) == P( Y &gt; 4) == P( Y &gt;= 5) 1-ppois(4, 2.3) ## [1] 0.08375072 5.7 Biến ngẫu nhiên liên tục Biến ngẫu nhiên liên tục (NNLT) có thể nhận số lượng vô hạn giá trị, dẫn đến nhiều sự khác biệt về toán học kahs hóc búa so với biến NNRR. Cụ thể, xác suất để biến NNLT \\(X\\) nhận giá trị \\(x\\) bằng 0, vì vậy chúng ta sẽ tìm xác suất biến \\(X\\) rơi vào một khoảng nhất định. Vì thế, thay vì lấy tổng xác suất, ta sẽ lấy tích phân. Do tài liệu này không chuyên sâu về toán nên thay vào đó ta ưu tiên sử dụng R để tính hơn, nhưng vẫn sẽ đi sâu vào 1 số công thức toán cần thiết để hiểu sâu về bản chất. 5.7.1 PDF, CDF và CCDF 5.7.1.1 PDF - Hàm mật độ xác suất Với các PP liên tục, ta sử dụng PDF (probability density function) thay vì PMF (probability mass function - hàm khối xác suất). PMF là hàm để tính xác suất của một biến ngẫu nhiên \\(X\\) nhận giá trị \\(x\\), tức là \\(P(X=x)\\). Đồng thời, tổng xác suất của mọi \\(x\\) mà \\(X\\) có thể nhận bằng 1. Khi biểu diễn trên đồ thị, ta thấy xác suất của mỗi tương ứng với mỗi giá trị \\(x\\) của biến NNRR \\(X\\) chính là chiều cao của của các bar. #insert a graph here to demonstrate!!! Tuy nhiên, với biến NNLT, do không thể tìm được xác suất của 1 giá trị \\(x\\) cụ thể, ta sẽ quan tâm tới phần diện tích bên dưới đồ thị, cụ thể là ở giữa một khoảng 2 giá trị \\([a,b]\\) nào đó, tương đương với xác suất mà biến \\(X\\) nhận giá trị trong khoảng này. Cũng cần lưu ý rằng, xác suất mà biến \\(X\\) nhận giá trị trong khoảng \\([a,b]\\) không sử dụng hàm PDF để tính. Hàm PDF chỉ dùng để biểu diễn mật độ xác suất. 5.7.1.2 CDF - Hàm phân phối tích luỹ Coi \\(f(x)\\) là một hàm PDF và \\(F(x)\\) là CDF tương ứng. Hàm \\(f(x)\\) có thể nhận giá trị bất kỳ từ 0 đến \\(\\infty\\), với điều kiện phần diện tích dưới \\(f(x)\\) bằng 1, i.e. tích phân của \\(f(x)\\) bằng 1, hay \\(\\int_{-\\infty}^{x} f(t) dt = 1\\). Khi muốn tìm xác suất từ hàm PDF, ta phải lấy tích phân của \\(f(x)\\) giữa 2 giá trị \\(a\\) và \\(b\\). Nếu giá trị bên trái, tức là \\(a = -\\infty\\), ta có CDF chính là \\(P(X \\leq b)\\). Ngược lại với CDF, ta còn quan tâm đến CCDF (complementary CDF) để tìm \\(P(X&gt;b)\\), hay chính là \\(1-P(X \\leq b) = 1 - CDF\\) 5.8 Các phân phối liên tục phổ biến 5.8.1 Exponential distribution - PP mũ PP mũ được sử dụng để mô hình hóa và dự đoán khoảng thời gian giữa 2 sự kiện liên tục, i.e. thời gian chờ đến sự kiện tiếp theo. Ví dụ: thời gian đến khi KH tiếp theo thanh toán thời gian chờ giữa 2 lần đón của xe buýt Tự định nghĩa này, có thể thấy PP mũ có liên quan chặt chẽ đến PP Poisson (tính xác suất số sự kiện thành công trong 1 đơn vị thời gian/không gian). PP Poisson cho ta biết về số sự kiện thành công trong mỗi khoảng thời gian, đồng thời PP mũ giúp ta miêu tả khoảng thời gian giữa mỗi sự kiện. 5.8.1.1 PDF của PP mũ PP mũ có PDF như sau: \\[f(t)=\\begin{cases} \\lambda e^{-\\lambda t} &amp; t\\ge0\\;\\textrm{ and }\\;\\lambda&gt;0\\\\ 0 &amp; \\textrm{otherwise} \\end{cases}\\] Có thể suy ra công thức PDF trên từ công thức PDF của PP Poisson. Hai phân phối mũ và Poisson đều bắt nguồn từ Poisson process, trong đó có \\(\\lambda\\) là event rate, i.e. số sự kiện trong mỗi đơn vị thời gian. Như vậy, trung bình sẽ có \\(\\lambda t\\) sự kiện xảy ra trong \\(t\\) đơn vị thời gian. Từ công thức PMF của Poisson: \\(\\textrm{Có } P(x) = e^{-\\lambda t}(\\lambda t)^x / x! \\\\ \\Leftrightarrow P(x=0) = e^{-\\lambda t} \\\\ \\textrm{Coi T là thời gian chờ đến sự kiện tiếp theo, ta có T &gt; t} \\equiv \\textrm{x = 0} \\\\ \\Leftrightarrow P(T&gt;t) = e^{-\\lambda t}\\\\ \\Leftrightarrow P(T \\leq t) = 1 - e^{-\\lambda t}\\) Đây là chính là CDF của PP mũ, tức là khi lấy đạo hàm, ta có PDF: \\[F(t) = \\int_{0}^{\\infty} \\lambda e^{-\\lambda t} dt = 1-e^{-\\lambda t}\\] Ở trên, ta đã có \\(\\lambda t\\) sự kiện xảy ra trong \\(t\\) đơn vị thời gian, tức là: \\(n = \\lambda t \\\\ \\Leftrightarrow t/n = 1/\\lambda\\) Hay diễn đạt bằng lời, ta có: thời gian trung bình đến khi sự kiện tiếp theo diễn ra là \\(1/\\lambda\\), tương đương với: \\[\\mu = E[X] = \\frac{1}_{\\lambda}\\] Đồng thời, phương sai của PP mũ là: \\[\\sigma^2 = Var[X] = \\int_{0}^{\\infty} (x-\\mu)^2 f(x) dx = \\dots = frac{1}_{\\lambda^2}\\] 5.8.2 Uniform [0,1] distribution 5.8.3 Normal distribution d "]
]
